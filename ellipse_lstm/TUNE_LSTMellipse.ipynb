{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13ec02b",
   "metadata": {},
   "source": [
    "### Tuning the LSTM model ... \n",
    "\n",
    "File copied from \"REAL\" model tuning for EOF on 8/4/2025.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9289865b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 22:44:46.485005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-30 22:44:46.516765: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-30 22:44:46.516786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-30 22:44:46.517793: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-30 22:44:46.523112: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-30 22:44:48.237129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78670 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:2d:00.0, compute capability: 9.0\n"
     ]
    }
   ],
   "source": [
    "##import cell makes its appearance once again...\n",
    "%matplotlib inline\n",
    "##so-called \"math\" related imports\n",
    "#from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import sample\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import xarray as xr\n",
    "\n",
    "import pickle\n",
    "\n",
    "##plotting related imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow/keras related files\n",
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "tf.device('/physical_device:GPU:0')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#import investigate\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9a5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4721c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input data, it is max/min standardized, NaNs removed\n",
    "#stratospheric polar vortex ellipse diagnostics from Fernandez et al 2025 (in review)\n",
    "#comparison here may be a little muddled because I am limitied in the days that I can observe, however, I am currently working(7/8/2025) on calculating the diagnostics for the full dataset\n",
    "#this will be fixed. \n",
    "\n",
    "infile = open(\"../data/nolag_extendedanom_input.p\", 'rb') \n",
    "nolag_input = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "nolag_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 149\n",
    "#switch to 149 for normal. \n",
    "\n",
    "inp1 = np.empty((62,idx,8)) #create new input array, 14 day lag. \n",
    "\n",
    "shift = 0\n",
    "#0 if normal\n",
    "#this indicates setting the start date as November 2 rather than October 19\n",
    "\n",
    "##reshaping to change lag in metrics for input\n",
    "inp1[:,:,0] = nolag_input[:62,shift:,0] ##wind\n",
    "inp1[:,:,1] = nolag_input[:62,shift:,1] ##ratio\n",
    "inp1[:,:,2] = nolag_input[:62,shift:,2] ##latitude\n",
    "inp1[:,:,3] = nolag_input[:62,shift:,3] ##longitude\n",
    "inp1[:,:,4] = nolag_input[:62,shift:,4] ##size\n",
    "inp1[:,:,5] = nolag_input[:62,shift:,5] ##ephi\n",
    "inp1[:,:,6] = nolag_input[:62,shift:,6] ##gph\n",
    "inp1[:,:,7] = nolag_input[:62,shift:,7] ##pv\n",
    "\n",
    "inp=inp1.reshape(62*idx,8)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472ba568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load output data file.\n",
    "#I can change this to represent any of the available temp regions. \n",
    "infile = open(\"../data/eur_anomtemps_reduced.p\",\"rb\",)\n",
    "output = pickle.load(infile) \n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5ff3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb774f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.771931</td>\n",
       "      <td>-1.799955</td>\n",
       "      <td>1.313722</td>\n",
       "      <td>-1.630033</td>\n",
       "      <td>0.566290</td>\n",
       "      <td>1.589922</td>\n",
       "      <td>-0.172617</td>\n",
       "      <td>1.566368</td>\n",
       "      <td>0.659295</td>\n",
       "      <td>0.199667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379306</td>\n",
       "      <td>1.641585</td>\n",
       "      <td>-0.743088</td>\n",
       "      <td>-0.070795</td>\n",
       "      <td>0.742423</td>\n",
       "      <td>0.454864</td>\n",
       "      <td>0.251855</td>\n",
       "      <td>0.795493</td>\n",
       "      <td>-0.417825</td>\n",
       "      <td>-1.029987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.803999</td>\n",
       "      <td>-1.900528</td>\n",
       "      <td>1.245334</td>\n",
       "      <td>-1.692496</td>\n",
       "      <td>0.101666</td>\n",
       "      <td>2.042962</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>1.506671</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>0.238603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347120</td>\n",
       "      <td>1.752066</td>\n",
       "      <td>-0.985741</td>\n",
       "      <td>0.049346</td>\n",
       "      <td>0.764168</td>\n",
       "      <td>0.594407</td>\n",
       "      <td>0.432133</td>\n",
       "      <td>1.080915</td>\n",
       "      <td>-0.165571</td>\n",
       "      <td>-1.420320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.872130</td>\n",
       "      <td>-2.032226</td>\n",
       "      <td>1.424598</td>\n",
       "      <td>-1.343837</td>\n",
       "      <td>0.161601</td>\n",
       "      <td>1.711995</td>\n",
       "      <td>0.503023</td>\n",
       "      <td>0.826614</td>\n",
       "      <td>-1.105172</td>\n",
       "      <td>0.270467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.585204</td>\n",
       "      <td>1.588777</td>\n",
       "      <td>-1.302961</td>\n",
       "      <td>0.097989</td>\n",
       "      <td>0.733291</td>\n",
       "      <td>0.651646</td>\n",
       "      <td>0.515277</td>\n",
       "      <td>0.912816</td>\n",
       "      <td>0.289325</td>\n",
       "      <td>-1.499576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.867914</td>\n",
       "      <td>-2.054812</td>\n",
       "      <td>1.657540</td>\n",
       "      <td>-0.838510</td>\n",
       "      <td>0.746273</td>\n",
       "      <td>1.227320</td>\n",
       "      <td>0.679350</td>\n",
       "      <td>1.134812</td>\n",
       "      <td>-0.876215</td>\n",
       "      <td>0.232273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789144</td>\n",
       "      <td>1.273398</td>\n",
       "      <td>-1.416257</td>\n",
       "      <td>-0.048546</td>\n",
       "      <td>0.671334</td>\n",
       "      <td>0.746395</td>\n",
       "      <td>0.300846</td>\n",
       "      <td>0.709486</td>\n",
       "      <td>0.123002</td>\n",
       "      <td>-0.751547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.787536</td>\n",
       "      <td>-2.011126</td>\n",
       "      <td>1.559197</td>\n",
       "      <td>-0.409157</td>\n",
       "      <td>1.239623</td>\n",
       "      <td>0.913107</td>\n",
       "      <td>0.641640</td>\n",
       "      <td>1.931937</td>\n",
       "      <td>-0.272880</td>\n",
       "      <td>0.127776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377190</td>\n",
       "      <td>1.043657</td>\n",
       "      <td>-1.369488</td>\n",
       "      <td>-0.219058</td>\n",
       "      <td>0.751703</td>\n",
       "      <td>1.057360</td>\n",
       "      <td>0.347496</td>\n",
       "      <td>0.404459</td>\n",
       "      <td>0.330464</td>\n",
       "      <td>0.215042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>0.415159</td>\n",
       "      <td>-0.170777</td>\n",
       "      <td>-0.531934</td>\n",
       "      <td>-0.397453</td>\n",
       "      <td>1.102362</td>\n",
       "      <td>-0.267771</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>1.148350</td>\n",
       "      <td>-2.241040</td>\n",
       "      <td>0.180624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.535923</td>\n",
       "      <td>1.136943</td>\n",
       "      <td>-0.416174</td>\n",
       "      <td>0.562862</td>\n",
       "      <td>-0.354710</td>\n",
       "      <td>-0.763675</td>\n",
       "      <td>-0.224326</td>\n",
       "      <td>0.846225</td>\n",
       "      <td>-1.504478</td>\n",
       "      <td>-0.403197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8366</th>\n",
       "      <td>0.484640</td>\n",
       "      <td>-0.232514</td>\n",
       "      <td>-0.597377</td>\n",
       "      <td>0.039627</td>\n",
       "      <td>0.837373</td>\n",
       "      <td>-0.276658</td>\n",
       "      <td>0.205001</td>\n",
       "      <td>1.321532</td>\n",
       "      <td>-2.205615</td>\n",
       "      <td>0.232497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.670336</td>\n",
       "      <td>1.115493</td>\n",
       "      <td>-0.641672</td>\n",
       "      <td>0.649173</td>\n",
       "      <td>-0.423895</td>\n",
       "      <td>-0.589386</td>\n",
       "      <td>-0.034272</td>\n",
       "      <td>0.739777</td>\n",
       "      <td>-1.244535</td>\n",
       "      <td>-0.653119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>0.558206</td>\n",
       "      <td>-0.163820</td>\n",
       "      <td>-0.476075</td>\n",
       "      <td>0.480464</td>\n",
       "      <td>0.879917</td>\n",
       "      <td>-0.589342</td>\n",
       "      <td>-0.360315</td>\n",
       "      <td>1.203000</td>\n",
       "      <td>-2.164044</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.172690</td>\n",
       "      <td>0.839839</td>\n",
       "      <td>-1.031493</td>\n",
       "      <td>0.910849</td>\n",
       "      <td>-0.425116</td>\n",
       "      <td>-0.327954</td>\n",
       "      <td>0.842602</td>\n",
       "      <td>1.100344</td>\n",
       "      <td>-0.612106</td>\n",
       "      <td>-0.448852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>0.591930</td>\n",
       "      <td>0.343880</td>\n",
       "      <td>-0.538673</td>\n",
       "      <td>1.014618</td>\n",
       "      <td>0.200438</td>\n",
       "      <td>-0.391132</td>\n",
       "      <td>-0.570871</td>\n",
       "      <td>1.115725</td>\n",
       "      <td>-1.033758</td>\n",
       "      <td>0.161263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.749049</td>\n",
       "      <td>0.472475</td>\n",
       "      <td>-1.311037</td>\n",
       "      <td>0.590795</td>\n",
       "      <td>-0.424628</td>\n",
       "      <td>-0.503666</td>\n",
       "      <td>0.747163</td>\n",
       "      <td>1.328334</td>\n",
       "      <td>-0.101280</td>\n",
       "      <td>-0.280176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>0.809561</td>\n",
       "      <td>0.772545</td>\n",
       "      <td>-0.603345</td>\n",
       "      <td>1.125781</td>\n",
       "      <td>-0.671348</td>\n",
       "      <td>0.540565</td>\n",
       "      <td>-0.407312</td>\n",
       "      <td>0.528993</td>\n",
       "      <td>-0.335991</td>\n",
       "      <td>0.178140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.634721</td>\n",
       "      <td>0.056345</td>\n",
       "      <td>-1.487699</td>\n",
       "      <td>0.103556</td>\n",
       "      <td>-0.648218</td>\n",
       "      <td>-0.887477</td>\n",
       "      <td>0.445942</td>\n",
       "      <td>1.081470</td>\n",
       "      <td>0.609084</td>\n",
       "      <td>0.025298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8370 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.771931 -1.799955  1.313722 -1.630033  0.566290  1.589922 -0.172617   \n",
       "1     0.803999 -1.900528  1.245334 -1.692496  0.101666  2.042962  0.076518   \n",
       "2     0.872130 -2.032226  1.424598 -1.343837  0.161601  1.711995  0.503023   \n",
       "3     0.867914 -2.054812  1.657540 -0.838510  0.746273  1.227320  0.679350   \n",
       "4     0.787536 -2.011126  1.559197 -0.409157  1.239623  0.913107  0.641640   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8365  0.415159 -0.170777 -0.531934 -0.397453  1.102362 -0.267771  0.062068   \n",
       "8366  0.484640 -0.232514 -0.597377  0.039627  0.837373 -0.276658  0.205001   \n",
       "8367  0.558206 -0.163820 -0.476075  0.480464  0.879917 -0.589342 -0.360315   \n",
       "8368  0.591930  0.343880 -0.538673  1.014618  0.200438 -0.391132 -0.570871   \n",
       "8369  0.809561  0.772545 -0.603345  1.125781 -0.671348  0.540565 -0.407312   \n",
       "\n",
       "            7         8         9   ...        52        53        54  \\\n",
       "0     1.566368  0.659295  0.199667  ... -0.379306  1.641585 -0.743088   \n",
       "1     1.506671  0.006843  0.238603  ...  0.347120  1.752066 -0.985741   \n",
       "2     0.826614 -1.105172  0.270467  ... -0.585204  1.588777 -1.302961   \n",
       "3     1.134812 -0.876215  0.232273  ...  0.789144  1.273398 -1.416257   \n",
       "4     1.931937 -0.272880  0.127776  ...  0.377190  1.043657 -1.369488   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8365  1.148350 -2.241040  0.180624  ... -0.535923  1.136943 -0.416174   \n",
       "8366  1.321532 -2.205615  0.232497  ... -0.670336  1.115493 -0.641672   \n",
       "8367  1.203000 -2.164044  0.126000  ... -1.172690  0.839839 -1.031493   \n",
       "8368  1.115725 -1.033758  0.161263  ... -0.749049  0.472475 -1.311037   \n",
       "8369  0.528993 -0.335991  0.178140  ... -0.634721  0.056345 -1.487699   \n",
       "\n",
       "            55        56        57        58        59        60        61  \n",
       "0    -0.070795  0.742423  0.454864  0.251855  0.795493 -0.417825 -1.029987  \n",
       "1     0.049346  0.764168  0.594407  0.432133  1.080915 -0.165571 -1.420320  \n",
       "2     0.097989  0.733291  0.651646  0.515277  0.912816  0.289325 -1.499576  \n",
       "3    -0.048546  0.671334  0.746395  0.300846  0.709486  0.123002 -0.751547  \n",
       "4    -0.219058  0.751703  1.057360  0.347496  0.404459  0.330464  0.215042  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8365  0.562862 -0.354710 -0.763675 -0.224326  0.846225 -1.504478 -0.403197  \n",
       "8366  0.649173 -0.423895 -0.589386 -0.034272  0.739777 -1.244535 -0.653119  \n",
       "8367  0.910849 -0.425116 -0.327954  0.842602  1.100344 -0.612106 -0.448852  \n",
       "8368  0.590795 -0.424628 -0.503666  0.747163  1.328334 -0.101280 -0.280176  \n",
       "8369  0.103556 -0.648218 -0.887477  0.445942  1.081470  0.609084  0.025298  \n",
       "\n",
       "[8370 rows x 62 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to pandas dataframe\n",
    "input = pd.DataFrame(inp)\n",
    "#label columns of variables for input data\n",
    "col_names = ['wind','rat','cenlat','cenlon','size','ephi','gph','pv']\n",
    "input.columns = col_names\n",
    "\n",
    "input##make pandas dataframe for RF\n",
    "input = pd.DataFrame(inputvar)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp2 = input.values.reshape(62,149,8)\n",
    "inp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb6ab905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 135)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##need to change this based on lag \n",
    "## 14-days = [:, 10:] 10 day window rather than 14\n",
    "## 20-days = [:, 16:]\n",
    "## 30-days = [:, 26:]\n",
    "\n",
    "# 139\n",
    "# 133\n",
    "# 123\n",
    "\n",
    "## 14-days = [:, 14:] 14 day window\n",
    "## 20-days = [:, 20:]\n",
    "## 30-days = [:, 30:]\n",
    "\n",
    "# 135\n",
    "# 129\n",
    "# 119\n",
    "\n",
    "lead = 135\n",
    "\n",
    "temp = output.reshape(62, 149)\n",
    "temp = temp[:, 14:]\n",
    "print(temp.shape)\n",
    "\n",
    "temp_flat = temp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "099fed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timeseries data arrays for PCs \n",
    "#was 122 and 14 for the 14 day window. \n",
    "\n",
    "new_input =  np.empty((62,lead,14,8))\n",
    "new_output = np.empty((62,lead))\n",
    "\n",
    "for i in range(0,62):\n",
    "    for j in range(0,lead):\n",
    "        new_input[i,j,:,:] = inp2[i,j:j+14,:]\n",
    "        new_output[i,j] = temp[i,j]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f63cce37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##create timeseries but select from 3 categories ...\n",
    "new_input = []\n",
    "new_output = []\n",
    "\n",
    "for i in range(14,9362):\n",
    "    print(i)\n",
    "    if temp_flat[i] == 0:\n",
    "        new_input.append(sele_ind_data[i-14:i,:])\n",
    "        new_output.append(temp_flat[i])\n",
    "    elif temp_flat[i] == 1:\n",
    "        new_input.append(sele_ind_data[i-14:i,:])\n",
    "        new_output.append(temp_flat[i])\n",
    "    if temp_flat[i] == 2:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8abac728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 121, 14, 10)\n",
      "(62, 121)\n"
     ]
    }
   ],
   "source": [
    "new_input = np.array(new_input)\n",
    "new_output = np.array(new_output)\n",
    "print(new_input.shape)\n",
    "print(new_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7245a2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_ind = round((62*lead)*0.15)\n",
    "frac_end = round((62*lead)-frac_ind)\n",
    "frac_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d5a18d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2bda666",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set X_all and Y_all datasets\n",
    "X_all = np.copy(new_input.reshape((62*lead),14,8))\n",
    "Y_all = np.copy(new_output.flatten())\n",
    "\n",
    "##training data partition out\n",
    "X_tri = X_all[:frac_end,:]\n",
    "Y_tri = Y_all[:frac_end]\n",
    "\n",
    "#testing data partition out\n",
    "X_tes = X_all[frac_end:,:]\n",
    "Y_tes = Y_all[frac_end:]\n",
    "\n",
    "#Convert the Y array into a categorical array. This means we will create one-hot vector labels for all of the inputs.\n",
    "# The one-hot vectors have an index for each possible output category (two in our case)\n",
    "# A \"1\" is put in the index corresponding to the category to which the sample belongs\n",
    "Y_all = keras.utils.to_categorical(Y_all)\n",
    "Y_tri = keras.utils.to_categorical(Y_tri)\n",
    "Y_tes= keras.utils.to_categorical(Y_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dccee110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7502, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25859b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking my data for NaN of Infs because I need to make sure this doesn't cause\n",
    "#the model to throw back no loss\n",
    "\n",
    "if np.any(np.isnan(X_all)) or np.any(np.isinf(X_all)):\n",
    "    print(\"NaN or Inf values found in X_all!\")\n",
    "\n",
    "if np.any(np.isnan(Y_all)) or np.any(np.isinf(Y_all)):\n",
    "    print(\"NaN or Inf values found in Y_all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf2d82",
   "metadata": {},
   "source": [
    "### Begin establishing specifics of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c90f45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7502, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking to make sure shape was properly one hot encoded\n",
    "Y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db9c2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "##class weight creator for the instance where now I have a four dimensional output array \n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    Y_reshaped = Y.reshape(-1, Y.shape[-1])\n",
    "    weights = np.max(np.sum(Y_reshaped, axis=0)) / np.sum(Y_reshaped, axis=0)\n",
    "    for i in range(Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90654be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##number of input nodes\n",
    "numb_int = X_all.shape[1:]\n",
    "\n",
    "##fraction of training data\n",
    "X_validation = X_tri[0:frac_ind]\n",
    "Y_validation = Y_tri[0:frac_ind]\n",
    "        \n",
    "X_train = X_tri[frac_ind:len(X_tri)]\n",
    "Y_train = Y_tri[frac_ind:len(Y_tri)]\n",
    "\n",
    "X_test = X_tes\n",
    "Y_test = Y_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd53f89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5252, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99997df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f361c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 2.1206179}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##do the class_dict weights\n",
    "class_weight = class_weight_creator(Y_train)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a672fb",
   "metadata": {},
   "source": [
    "### Model Architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c688082",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "batch_size = 64 #The number of samples the network sees before it backpropagates (batch size)\n",
    "epochs = 30 #The number of times the network will loop through the entire dataset (epochs)\n",
    "shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b566941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_LSTM(hp): \n",
    "    ##set ranges for hyperparameters\n",
    "    n1 = hp.Int('n1', min_value=32, max_value=160, step=32)\n",
    "    n2 = hp.Int('n2', min_value=4, max_value=32, step=4)\n",
    "    n3 = hp.Int('n3', min_value=8, max_value=32, step=8)\n",
    "    \n",
    "    reg1 = hp.Float('reg1', min_value=0.001, max_value=0.9, step=0.01)\n",
    "    reg2 = hp.Float('reg2', min_value=0.001, max_value=0.9, step=0.01)\n",
    "    reg3 = hp.Float('reg3', min_value=0.001, max_value=0.9, step=0.01)\n",
    "    \n",
    "    \n",
    "    input_tensor = Input(shape=(14, 8))\n",
    "    layer1 = layers.RNN(\n",
    "        layers.LSTMCell(n1, activation='tanh', use_bias=True,\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg1)),\n",
    "        return_sequences=True)(input_tensor)\n",
    "    layer2 = layers.RNN(\n",
    "        layers.LSTMCell(n2, activation='tanh', use_bias=True,\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg2)),\n",
    "                        return_sequences=False)(layer1)\n",
    "    layer3 = layers.Dense(n3, activation='relu',use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2=reg3))(layer2)\n",
    "\n",
    "    output_tensor = layers.Dense(2, activation='softmax',)(layer3)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    \n",
    "    lr = hp.Float('learning_rate', min_value=0.001, max_value=0.1)\n",
    "    \n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "    #decay_rate = lr / epochs\n",
    "    #momentum = 0.9\n",
    "\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=[keras.metrics.categorical_accuracy],)\n",
    "                            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c10bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "##the LSTM layers will have EXACTLY the same regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9c962b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "767dd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49c8daf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuning/EOF_seus1/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "#intialize/setup the hyperband tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    tune_LSTM, #model\n",
    "    objective = [kt.Objective('val_loss', 'min')],#kt.Objective('loss', 'min'),kt.Objective('val_categorical_accuracy', 'max')],\n",
    "    max_trials=1000, #number of iterations of tuning to run\n",
    "    max_consecutive_failed_trials=3, #number of allowed failed trials\n",
    "    directory = 'tuning',\n",
    "    project_name='ellipse_lstm1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b999bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595170a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 946 Complete [00h 00m 17s]\n",
      "multi_objective: 0.6452176570892334\n",
      "\n",
      "Best multi_objective So Far: 0.6266786456108093\n",
      "Total elapsed time: 06h 55m 26s\n",
      "\n",
      "Search: Running Trial #947\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "32                |64                |n1\n",
      "24                |32                |n2\n",
      "24                |16                |n3\n",
      "0.66              |0.46              |reg1\n",
      "0.45              |0.04              |reg2\n",
      "0.24              |0.47              |reg3\n",
      "0.076533          |0.086637          |learning_rate\n",
      "\n",
      "Epoch 1/50\n",
      "83/83 [==============================] - 3s 22ms/step - loss: 3.4607 - categorical_accuracy: 0.5061 - val_loss: 0.6903 - val_categorical_accuracy: 0.6880\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - 1s 18ms/step - loss: 0.9465 - categorical_accuracy: 0.4895 - val_loss: 0.6748 - val_categorical_accuracy: 0.6880\n",
      "Epoch 3/50\n",
      "83/83 [==============================] - 1s 17ms/step - loss: 0.9469 - categorical_accuracy: 0.4770 - val_loss: 0.6907 - val_categorical_accuracy: 0.6880\n",
      "Epoch 4/50\n",
      "83/83 [==============================] - 1s 17ms/step - loss: 0.9436 - categorical_accuracy: 0.5345 - val_loss: 0.6950 - val_categorical_accuracy: 0.3120\n",
      "Epoch 5/50\n",
      "83/83 [==============================] - 1s 18ms/step - loss: 0.9479 - categorical_accuracy: 0.4823 - val_loss: 0.6699 - val_categorical_accuracy: 0.6880\n",
      "Epoch 6/50\n",
      "83/83 [==============================] - 1s 17ms/step - loss: 0.9460 - categorical_accuracy: 0.5139 - val_loss: 0.7056 - val_categorical_accuracy: 0.3120\n",
      "Epoch 7/50\n",
      "83/83 [==============================] - 1s 17ms/step - loss: 0.9439 - categorical_accuracy: 0.5276 - val_loss: 0.6721 - val_categorical_accuracy: 0.6880\n",
      "Epoch 8/50\n",
      "83/83 [==============================] - 1s 17ms/step - loss: 0.9451 - categorical_accuracy: 0.4556 - val_loss: 0.6973 - val_categorical_accuracy: 0.3120\n",
      "Epoch 9/50\n",
      "83/83 [==============================] - 1s 18ms/step - loss: 0.9444 - categorical_accuracy: 0.4895 - val_loss: 0.7002 - val_categorical_accuracy: 0.3120\n",
      "Epoch 10/50\n",
      " 1/83 [..............................] - ETA: 1s - loss: 0.9478 - categorical_accuracy: 0.3281"
     ]
    }
   ],
   "source": [
    "##actual tuning process\n",
    "tuner.search(X_train, Y_train, validation_data=(X_validation, Y_validation),\n",
    "             batch_size=batch_size, epochs=epochs, shuffle=shuffle,\n",
    "             class_weight = class_weight, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acd9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "lstm_model = tuner.hypermodel.build(best_hps)\n",
    "history = lstm_model.fit(X_train,Y_train, validation_data=(X_validation, Y_validation),\n",
    "                    batch_size=batch_size, epochs=epochs, shuffle=shuffle,class_weight = class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=lstm_model.history.history['loss']\n",
    "val_loss=lstm_model.history.history['val_loss']\n",
    "    \n",
    "cat_acc=lstm_model.history.history['categorical_accuracy']\n",
    "val_acc=lstm_model.history.history['val_categorical_accuracy']\n",
    "        \n",
    "pred = lstm_model.predict(X_all)\n",
    "pred_val = lstm_model.predict(X_validation)\n",
    "pred_train = lstm_model.predict(X_train)\n",
    "pred_test = lstm_model.predict(X_test)\n",
    "\n",
    "# Look at the optimization history\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_loss, label='Training loss')\n",
    "ax1.plot(val_loss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cat_acc, label='Training ACC')\n",
    "ax2.plot(val_acc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c16efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
