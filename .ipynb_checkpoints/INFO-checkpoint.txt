7/8/2025

I created this repo to show my work from the Lessons Learned paper/play around a little bit with some different data.
The OG data only covers Nov-Mar, 1959/1960 to 2020/2021.
I changed it to extend to Oct-Mar for the normalizing/averaging for testing purposes/to extend the data some. Not sure if this was necessarily needed?

Model skill did not necessarily change with this modification. All it really did was change the importance of GPH. 
Prob distributions are confusing. 
I will have to discuss these with Zheng. Maybe it will be more worth it to not do a LL paper?

As long as the SHAP plots make sense, I will probably stick with the back-extended data just for my own mental sanity and consistency. 

########################################################
7/9/2025

Okay, so after some deliberation ... it seems we have reached a consensus on what is going to happen here. 
My work is re-aligning with my WINGS proposal. It is just a matter of how exactly to skin the proverbial cat. 

The short story is that the FOR from the RF model, particularly when switching forecast regions, increase substantially. The model performs better than climo with >0 BSS. This does not change dramatically when comparing the two models ... which drives home Zheng's point of "why use a more complicated model if you do not have to"

The only thing that changed was the extent of the input data, adding in ~8% of data, and a shifting of the running mean to earlier. This caused GPH to increase in importance, likely attributed to the model favoring that tropospheric relationship. I will need to discuss this with Andrea. I do not think there is a direct NAO teleconnection observed here because it is not physically consistent. 
However, in more fascinating observations, the distributions of features are the same for BOTH forecast areas. This is GREAT. It means that when the model is confident for both regions, its for the same reasons. 
:)

Same with SE US, just checked.

########################################################
7/10/2025

I am briefly going to test all of this using anomalies instead of just running mean values. 
I also switched GPH to 100 hPa. 

nolag_extendedanom_input

Nothing really changed except the features. 

########################################################
7/11/2025

Changed the validation and testing windows to 15% eac or ~10 years to match what was used for the EOF model. 

There is some overfitting in the European model before conducting feature selection. This is less prevalent for the other regions. After feature selection, model overfitting is reduced. 

All models look good. Evidence for increased instances for opportunity forecasts. 

Now I am going to redo the boxplots with the increased window. Hopefully to compare with the EOF model. 

My initial comparison with the EOF model shows consistent FOs from the 100hPa GPH, which is the most important feature for the RF model, however the information from the U-wind is different. Almost opposite. It's almost directly showing that the EOF model does better at picking up strong vortex signals connected with positive temperature anomalies, same with negative and a disrupted vortex. It is almost like the EOF model does a better job at picking up known sources of stratospheric variability and connecting that to skillful predictions of temperature anomalies... but the RF is picking up on other non-specific stratospheric varaibilities related to the vortex itself and its location, etc. It gets more information from the vortex behavior and uses that to make decisions, and those are the same regardless of region. 

I would like to see whether the EOF FOs change with forecast region. 
I will try that on monday. 
After that, I will write the outline for this new LL paper and begin work on expanding the previous one. 

########################################################
7/14/2025

I am trying to test whether the FOs change within the EOF LSTM by switching to the Canadian forecast region. 

########################################################
7/15/2025

And now I would like to see how many of these events overlap between the LSTM and RF models. 

########################################################
7/16/2025

I have a thought. 
I am currently unsure whether or not it makes sense that the FOs show the same thing. This could be a good thing. This could imply that there is some other type of stratospheric variability that is crucial in improving forecasts in this observed range with our features... OR it could be wrong. I am feeling motivated to look again at shifting the temps/inputs forward to November for such a prediction to see if that changes the FOs and model skill. This will also make comparing indices with the other model easier, selfishly. 

Okay, so think about it this way. 
The EOF model is October 19-March ... whatever. It also ends on date index 168 but from the 14 day window used for the LSTM. So think about it, end to end, as Index 33 to Index 168 as being the first last day of the windows and last ... last day of the windows. AHAHA XD. So when you switch to this RF model, you are just looking at the 1-to-1 comparison. November 2 forecasting for November 16 ... through to the end of March. This case creates output target arrays of the same shape (62*135,2). 

NOW... they match. 
I think I am going to keep this "reduced" RF set up, make the plots, etc. 

I also fixed the ways the box plots were indexed and created, so I will need to resend that image to Andrea. 

My real question is still WHY the FOs are the same regardless of region. 
Is this a good thing? 

I wrote a little list of things for me to double-check tomorrow before getting involved with looking at the composites again. (The dates of which should match now...)

########################################################
7/17/2025
All checked out. Images were correct.

Now I am working on the comparisons of the indexes between the two models. I have that code working, I believe. I have it saving the % overalp between models and # of shared events.
I went in and plan to save the indexes of the events themselves so that I can look at composites of all the things (box plots and cross sections) so we can see what the common features are and not!

I am saving these shared indices to the `shared_keys` folder. 

I have them done ... Not quite sure what is happening to positive and correct U wind SEUS though...

########################################################
7/18/2025

I plan to look further into why the SEUS plot is messed up later (@ gym) + throw together comparison PowerPoint for advisors. 

Right now I am working on the outline for this updated LL paper. 

########################################################
7/22/2025

I have been working on the updated LL paper, hence the lack of activity. 

########################################################
7/23/2025

I tried the EOF model as a RF model.
I think the results are comparable. I felt compelled to even try because it was looking like day 0 (14 days out) in the SHAP density plot was highest, so I felt it may not make too much of a difference to just try the model as a RF isntead for simplicity... also because I feel like it is something that reviewers would comment on in the LL paper. Using the same architecture. 

Same results as LSTM. Ellipse metrics are still better for FOs. 

########################################################
7/25/2025

Putting together figures for comparison. 

########################################################
7/31/2025

I have been working on paper edits for Fernandez et al 2025 to JAMC. 
I cam back to do a fourth ellipse model, LSTM. Just to come full circle with the model comparison. 

Forgot to change config again. Lol. 

########################################################
8/4/2025

Got sick. Had to take a break. I am back now to fix the LSTM ellipse model, conduct some proper tuning. 
I added in a .gitignore to leave out those files from the repo because there are about to be a large number of them. 

########################################################
8/5/2025

SkillScore run for tuned LSTM ellipse model. Interesting results. Looks like for confident predictions it is NEVER predicting negative temperature outcomes correctly/on purpose. Not sure why. 

I wonder still if this is a tuning issue on my part of whether this model really is just dogsh*t.

########################################################
8/6/2025

Apparently my logic was backwards for false negative/false positive. Which I kinda already knew? I just did not think it was as confusing. I am going to need to go back in and remediate this because ... it's wrong, obviously. This means fixing and re-running all of the models. Yay me. 

This all came about because I was trying to distinguish how many events where near neutral for classification within the RF's decision making. 
And it does look like this is the case ... 
The confident and correct events are predominantly that classification whereas false pos/neg has more neutral events.

########################################################
8/8/2025

Updated comparisons of indices are in paper draft. 

########################################################
8/14/2025

I was working on manuscript edits for a separate paper. 
Those are mostly done, so now I am going to be tuning details for this work so that I can start writing. 
I am starting by looking at the distribution of >1.5 std temp events (mild anomalies) that coincided with 90th percentile predictions vs. neutrals.

I will also be modifying and saving box plots from skill scores, potentially. 

########################################################
8/20/2025

I was back in here to do some image processing and whatnot for this paper and I have kinda reached a point where ... like. I think I have what I need. 
To a certain extent, I feel like I am procrastinating the writing component ahahaha. 
Essentially what I wanted to do was see if there were any obvious differences between the actual ellipse values for temp anomalies in making confident and correct predicitions, and what I found was that, really, no?
Especially for the ellipse diagnostics.

I feel like what this tells us is that when it comes to making these confident predictions, it really is more about what the STRATOSPHERE is doing at those moments rather than the actual temperature value/outcome. 
Given that there is consistency, regardless of whether the temperature is actually extreme (or not), we could be motivated to say that the stratosphere is the real driver in this decision making and the cause for increased confidence in place of the outcome ITSELF.
There is no tropospheric driver to the decision making of these models when they are highly confident ... it is learning purely from the behavior of these stratospheric diagnostics.  

AT LEAST FOR THE RF MODELS....

Now I am gonna look at this for the LSTMs because I wanna know if they also only use stratospheric info. lol. 

########################################################
8/28/2025

I have done quite a bit with the explainability of this model by looking into what exactly is causing the ellipse RF to have increased explainability. 
Well, thanks to thoughts that plagued me at 2AM, I revisited the RF ellipse model and ran several different iterations without certain metrics as a means to evaluate what exactly is giving the model those windows of opportunity. 
As it turns out, what I realized is that the model observes increased forecasts of opportunity due to combined information from 
1) the vortex location relative to an upper atmospheric wave-1 signal and 
2) how that communicates with climate modes (NAO and AO) as shown through PV and GPH, with the GPH response being greater. 

This is very interesting and important, and I am working on uploading some additional data here so that I can continue to make some cross sections and plot for explainability. 

1) full field 100hPa cross section of GPH over the North Atlantic (1959_gphNA_2deg_100)
2) full field 350K PV polar cap cross section (pv350pt)
3) updated ellipse diags with 350K PV
4) CAP_pvu_weightedANOM_350 (timeseries)
5) NA_gph_weightedANOM_100 (timeseries)
6) 1959_gphforNAO (80-40 lat averaged, all lons and levels for cross section post-RF)

I am quickly going to re-run the RF with the updated ellipse diags, but I do not think that will make a difference. 

PV and GPH
Average Num. of 10% Confident and Correct Postive Predictions: 30.23%
Average Num. of 10% Confident and Correct Negative Predictions: 30.87%
Average Num. of 10% Confident and FALSE Postive Predictions: 19.73%
Average Num. of 10% Confident and FALSE Negative Predictions: 19.17%
#######################################################################
Average Num. of 10% Confident and Correct Predictions: 61.10%
Average Num. of 10% Confident and FALSE Predictions: 38.90%

PV and Ellipse
Average Num. of 10% Confident and Correct Postive Predictions: 35.37%
Average Num. of 10% Confident and Correct Negative Predictions: 34.31%
Average Num. of 10% Confident and FALSE Postive Predictions: 14.63%
Average Num. of 10% Confident and FALSE Negative Predictions: 15.69%
#######################################################################
Average Num. of 10% Confident and Correct Predictions: 69.68%
Average Num. of 10% Confident and FALSE Predictions: 30.32%

Just Ellipse
Average Num. of 10% Confident and Correct Postive Predictions: 37.03%
Average Num. of 10% Confident and Correct Negative Predictions: 35.58%
Average Num. of 10% Confident and FALSE Postive Predictions: 12.98%
Average Num. of 10% Confident and FALSE Negative Predictions: 14.42%
#######################################################################
Average Num. of 10% Confident and Correct Predictions: 72.60%
Average Num. of 10% Confident and FALSE Predictions: 27.40%

Just GPH
Average Num. of 10% Confident and Correct Postive Predictions: 33.22%
Average Num. of 10% Confident and Correct Negative Predictions: 29.32%
Average Num. of 10% Confident and FALSE Postive Predictions: 17.70%
Average Num. of 10% Confident and FALSE Negative Predictions: 19.76%
#######################################################################
Average Num. of 10% Confident and Correct Predictions: 62.54%
Average Num. of 10% Confident and FALSE Predictions: 37.46%

GPH and Ellipse
Average Num. of 10% Confident and Correct Postive Predictions: 42.39%
Average Num. of 10% Confident and Correct Negative Predictions: 36.61%
Average Num. of 10% Confident and FALSE Postive Predictions: 7.60%
Average Num. of 10% Confident and FALSE Negative Predictions: 13.39%
#######################################################################
Average Num. of 10% Confident and Correct Predictions: 79.00%
Average Num. of 10% Confident and FALSE Predictions: 21.00%

_____________________________________________________________________
Brier Skill Score (All): 0.0661
Brier Skill Score (Train): 0.0715
Brier Skill Score (Test): 0.0822
Brier Skill Score (Validation): 0.0236
_____________________________________________________________________
Recall and Precision: Neg Cat
#########
Recall Accuracy Score (All): 0.7524
Recall Accuracy Score (Train): 0.7510
Recall Accuracy Score (Test): 0.7919
Recall AccuracyScore (Validation): 0.7204
Precision Accuracy Score (All): 0.6020
Precision Accuracy Score (Train): 0.6115
Precision Accuracy Score (Test): 0.5893
Precision AccuracyScore (Validation): 0.5755
_____________________________________________________________________
Recall and Precision: Pos Cat
#########
Recall Accuracy Score (All): 0.4675
Recall Accuracy Score (Train): 0.4810
Recall Accuracy Score (Test): 0.4495
Recall AccuracyScore (Validation): 0.4312
Precision Accuracy Score (All): 0.6390
Precision Accuracy Score (Train): 0.6409
Precision Accuracy Score (Test): 0.6854
Precision AccuracyScore (Validation): 0.5929

########################################################
9/2/2025
I am working on further developing this argument of ... ya know, ellipse metric viability. It is looking like everything is consistent+makes sense.
I added in the EHF full-field to look for any downward activity, I will have to show these to Andrea+Zheng. 

I think next I will look at these same composited but for those select temp events ... which is a lot of work.
But I can manage it, I think. 

I also want to explore a test model that has MSLP involved. (I gave this its own file.)