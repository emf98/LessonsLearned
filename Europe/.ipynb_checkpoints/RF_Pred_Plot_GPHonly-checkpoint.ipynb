{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70833382",
   "metadata": {},
   "source": [
    "### File for creating \"preceding  metrics\" plots for predictions using the RF model I trained ...\n",
    "## Europe.\n",
    "\n",
    "Updated 8/28/2025 ... This time, new file to just do vertical GPH cross sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa72bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxr\u001b[39;00m \n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "#relevant import statements\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import xarray as xr \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df424a55",
   "metadata": {},
   "source": [
    "Open and pre-process ellipse metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331391d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##open ellipse metric files\n",
    "infile = open(\"../data/ellipse/wind10_redo.p\", 'rb') \n",
    "wind10 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/ellipse/size10_redo.p\", 'rb') \n",
    "size10 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/ellipse/ratio10_redo.p\", 'rb') \n",
    "rat10 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/ellipse/ephi10_redo.p\", 'rb') \n",
    "ephi10 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/ellipse/cenlat10_redo.p\", 'rb')\n",
    "cenlat10 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/ellipse/cenlon10_redo.p\", 'rb')\n",
    "cenlon10 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/gph/NA_gph_weightedANOM_100.p\", 'rb') \n",
    "gph = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../data/pv/CAP_pvu_weightedANOM_350.p\", 'rb') \n",
    "pv = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6200163",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#remove leap year\n",
    "##CHANGE IF NOT EXTENDED DATAQ BACK TO 120\n",
    "wind10 = np.delete(wind10[:62],[151],1)\n",
    "rat10 = np.delete(rat10[:62],[151],1)\n",
    "cenlat10 = np.delete(cenlat10[:62],[151],1)\n",
    "cenlon10 = np.delete(cenlon10[:62],[151],1)\n",
    "size10 = np.delete(size10[:62],[151],1)\n",
    "ephi10 = np.delete(ephi10[:62],[151],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc574e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99358d70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "shift = 14\n",
    "#this is used to change the start date from October 19 to November 2nd... and just reduce the overall time observed.\n",
    "#0 if no change. \n",
    "\n",
    "idx = 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f59d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wind10 = wind10[:,19+shift:168]\n",
    "rat10 = rat10[:,19+shift:168]\n",
    "cenlat10 = cenlat10[:,19+shift:168]\n",
    "cenlon10 = cenlon10[:,19+shift:168]\n",
    "size10 = size10[:,19+shift:168]\n",
    "ephi10 = ephi10[:,19+shift:168]\n",
    "gph = gph[:62,19+shift:168]\n",
    "pv = pv[:62,19+shift:168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1eba8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove NaNs\n",
    "##CHANGE IF NOT EXTENDED DATA\n",
    "test_comp = []\n",
    "indices = np.isnan(wind10)\n",
    "for i in range(0,62):\n",
    "    for j in range(0,idx):\n",
    "        if indices[i,j] != False:\n",
    "            print(i)\n",
    "            print(j)\n",
    "            print(\"True\")\n",
    "            wind10[i,j] = 0\n",
    "            rat10[i,j] = 0\n",
    "            cenlat10[i,j] = 0\n",
    "            cenlon10[i,j] = 0\n",
    "            size10[i,j] = 0\n",
    "            ephi10[i,j] = 0\n",
    "            if i >= 57:\n",
    "                test_comp.append((i,j))\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dcdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.empty((62,idx,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d35f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def daily_anomaly(target):\n",
    "    dailymean = np.nanmean(target,axis=1)\n",
    "    anom = np.zeros_like(target)\n",
    "    for t in np.arange(target.shape[1]):\n",
    "         anom[:,t] = target[:,t] - dailymean\n",
    "    print(anom.shape)\n",
    "    return anom; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc53fcf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input[:,:,0] = daily_anomaly(wind10[:,:])\n",
    "input[:,:,1] = daily_anomaly(rat10[:,:])\n",
    "input[:,:,2] = daily_anomaly(cenlat10[:,:])\n",
    "input[:,:,3] = daily_anomaly(cenlon10[:,:])\n",
    "input[:,:,4] = daily_anomaly(size10[:,:])\n",
    "input[:,:,5] = daily_anomaly(ephi10[:,:])\n",
    "input[:,:,6] = daily_anomaly(gph[:,:])\n",
    "input[:,:,7] = daily_anomaly(pv[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = np.empty((62,idx,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input2[:,:,2] = cenlat10[:62,:]\n",
    "input2[:,:,1] = cenlon10[:62,:]\n",
    "input2[:,:,3] = wind10[:62,:]\n",
    "input2[:,:,0] = gph[:62,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191cb82",
   "metadata": {},
   "source": [
    "Now that the assembly is done ... We can plot. lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##open indice files\n",
    "infile = open(\"posXtest_eur14.p\", 'rb') \n",
    "posXtest = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"FposXtest_eur14.p\", 'rb') \n",
    "FposXtest = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"negXtest_eur14.p\", 'rb') \n",
    "negXtest = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"FnegXtest_eur14.p\", 'rb') \n",
    "FnegXtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ae998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in GPH data. \n",
    "infile = open(\"../data/gph/GPH7560_weightedANOM.p\",\"rb\")\n",
    "gph_input = pickle.load(infile)  ##GPH vertical cross section along longitudes\n",
    "gph_input = np.delete(gph_input,[151],1)\n",
    "GPH_14= gph_input[52:62, 19+shift:168, :, :].reshape((10, 135, 37, 180)) \n",
    "infile.close()\n",
    "\n",
    "#print(\"U Wind shape: \",U_14.shape)\n",
    "print(\"GPH shape: \",GPH_14.shape)\n",
    "\n",
    "lon = np.arange(0, 362, 2)\n",
    "print(\"Len longitudes: \", len(lon))\n",
    "\n",
    "lev = np.array([1., 2., 3., 5., 7., 10., 20., 30., 50., 70., 100., 125., 150., 175., 200., 225., 250., 300., 350., 400., \n",
    "                450., 500., 550., 600., 650., 700., 750., 775., 800., 825., 850., 875., 900., 925., 950., 975., 1000.])\n",
    "print(\"Len levels: \", len(lev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fd066",
   "metadata": {},
   "source": [
    "### These few cells will be for making box plots instead lol. \n",
    "\n",
    "It is important to keep in mind that there are also just proportionally less pos and neg temp extremes left as I increase the classification criterion (i.e., +1.5 std) ... SO regardless of whether they are proportionally more events, there are still less to choose from ... if that makes sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c298dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reduce input to just the testing data\n",
    "nolag_Xtest = input2[52:,:,:]\n",
    "nolag_Xtest.shape\n",
    "\n",
    "####now I wanna make these plots SO ... I am adding an index column on to X_test ... full version. \n",
    "ranges = np.array([x for x in range(0,idx*10,1)])\n",
    "ranges = ranges.reshape(10,idx) \n",
    "ranges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correct Positive Events\n",
    "posXtest_set = set(posXtest)\n",
    "\n",
    "pos_corr_events = []\n",
    "pos_corr_total_events = []\n",
    "GPH_cpos = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    for j in range(0,idx):\n",
    "        #index for the date being observed\n",
    "        date_index = ranges[i,j]\n",
    "        if date_index not in posXtest_set:\n",
    "            continue\n",
    "        elif date_index in posXtest_set:\n",
    "            features = nolag_Xtest[i, j, :]\n",
    "            pos_corr_events.extend(features)\n",
    "\n",
    "            GPH_cpos.extend(GPH_14[i,j,:,:])\n",
    "            \n",
    "            pos_corr_total_events.append(0)\n",
    "        \n",
    "##reshape\n",
    "Tpos = np.array(pos_corr_events).reshape(len(pos_corr_total_events),4)\n",
    "GPH_cpos = np.array(GPH_cpos).reshape(len(pos_corr_total_events),37,180)\n",
    "print(Tpos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce75a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FALSE Positive Events\n",
    "FposXtest_set = set(FposXtest)\n",
    "\n",
    "Fpos_corr_events = []\n",
    "Fpos_corr_total_events = []\n",
    "\n",
    "GPH_Fpos = []\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    for j in range(0,idx):\n",
    "        #index for the date being observed\n",
    "        date_index = ranges[i,j]\n",
    "        if date_index not in FposXtest_set:\n",
    "            continue\n",
    "        Fpos_corr_total_events.append(0)\n",
    "        Fpos_corr_events.extend(nolag_Xtest[i,j,:])\n",
    "        GPH_Fpos.extend(GPH_14[i,j,:,:])\n",
    "        \n",
    "##reshape\n",
    "Fpos = np.array(Fpos_corr_events).reshape(len(Fpos_corr_total_events),4)\n",
    "GPH_Fpos = np.array(GPH_Fpos).reshape(len(Fpos_corr_total_events),37,180)\n",
    "print(Fpos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correct Negative Events\n",
    "negXtest_set = set(negXtest)\n",
    "\n",
    "neg_corr_events = []\n",
    "neg_corr_total_events = []\n",
    "\n",
    "GPH_cneg = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    for j in range(0,idx):\n",
    "        #index for the date being observed\n",
    "        date_index = ranges[i,j]\n",
    "        if date_index not in negXtest_set:\n",
    "            continue\n",
    "        neg_corr_total_events.append(0)\n",
    "        neg_corr_events.extend(nolag_Xtest[i,j,:])\n",
    "        GPH_cneg.extend(GPH_14[i,j,:,:])\n",
    "        \n",
    "##reshape\n",
    "Tneg = np.array(neg_corr_events).reshape(len(neg_corr_total_events),4)\n",
    "GPH_cneg = np.array(GPH_cneg).reshape(len(neg_corr_total_events),37,180)\n",
    "print(Tneg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35aec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FALSE Negative Events\n",
    "FnegXtest_set = set(FnegXtest)\n",
    "\n",
    "Fneg_corr_events = []\n",
    "Fneg_corr_total_events = []\n",
    "GPH_Fneg = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    for j in range(0,idx):\n",
    "        #index for the date being observed\n",
    "        date_index = ranges[i,j]\n",
    "        if date_index not in FnegXtest_set:\n",
    "            continue\n",
    "        Fneg_corr_total_events.append(0)\n",
    "        Fneg_corr_events.extend(nolag_Xtest[i,j,:])\n",
    "        GPH_Fneg.extend(GPH_14[i,j,:,:])\n",
    "        \n",
    "##reshape\n",
    "Fneg = np.array(Fneg_corr_events).reshape(len(Fneg_corr_total_events),4)\n",
    "GPH_Fneg = np.array(GPH_Fneg).reshape(len(Fneg_corr_total_events),37,180)\n",
    "print(Fneg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 18\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 16))\n",
    "plt.suptitle(\"Composites of 60-75 $^o$N GPH Anomalies during 90th Percentile Confident Predictions, Ellipse RF\",fontsize=21)   \n",
    "\n",
    "titles = [\"True Positive\", \"True Negative\", \"False Positive\", \"False Negative\",]\n",
    "data = [GPH_cpos[:,5:],GPH_cneg[:,5:],\n",
    "        GPH_Fpos[:,5:],GPH_Fneg[:,5:]]\n",
    "\n",
    "axes = axes.flatten()\n",
    "#remove the last (empty) axis\n",
    "\n",
    "for i in range(0, 4):\n",
    "    color = \"RdBu_r\"\n",
    "    # colorbar options:\n",
    "    \n",
    "    colorbarMin = -200\n",
    "    colorbarMax = 200\n",
    "    colorspace = 20\n",
    "\n",
    "    clevel = np.arange(colorbarMin, colorbarMax + colorspace, colorspace)\n",
    "    axes[i].set_title(\"GPH, \"+str(titles[i]), fontsize=fs-1, y=0.99) \n",
    "\n",
    "    h = axes[i].contourf(\n",
    "        lon[:180],\n",
    "        lev[5:],\n",
    "        np.nanmean(data[i], axis = 0),\n",
    "        clevel,\n",
    "        cmap=color,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    cbar = plt.colorbar(\n",
    "        h, orientation=\"vertical\", shrink=1, fraction=0.1, pad=0.1, aspect=40\n",
    "    )\n",
    "    cbar.ax.tick_params(labelsize=fs-2)\n",
    "    axes[i].tick_params(labelsize=fs-2)\n",
    "    axes[i].set_yscale('log')\n",
    "    axes[i].invert_yaxis()\n",
    "    axes[i].set_ylabel('Pressure (hPa)', fontsize=fs-3)\n",
    "    axes[i].set_yticks([10, 30, 100, 200, 300, 450, 700, 1000]) \n",
    "    axes[i].get_yaxis().set_major_formatter(plt.ScalarFormatter()) \n",
    "\n",
    "    axes[i].set_xlim(0, 360)\n",
    "    axes[i].set_xlabel('Longitude', fontsize=fs-3)\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.945)\n",
    "#plt.savefig(\"EllipseRF_vertGPHComp.png\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d054fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
